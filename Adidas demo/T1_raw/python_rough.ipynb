{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType,DoubleType,IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOM=pd.DataFrame()\n",
    "PO=pd.DataFrame()\n",
    "Location_master=pd.DataFrame()\n",
    "sale_data=pd.DataFrame()\n",
    "port_delay_df=pd.DataFrame()\n",
    "financial=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOM['product_id'] = ''\n",
    "BOM['component_id'] = ''\n",
    "PO['component_id'] = ''\n",
    "PO['product_id'] = ''\n",
    "PO['supplier_name'] = ''\n",
    "PO['needed_date/lead_date'] = ''\n",
    "Location_master['supplier_name'] = ''\n",
    "Location_master['address'] = ''\n",
    "port_delay_df['location']=''\n",
    "port_delay_df['delay']=''\n",
    "port_delay_df['threshold/actual']=''\n",
    "financial['supplier_name']=''\n",
    "financial['financial_indicator']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplier_location=pd.merge(PO,Location_master,on=['supplier_name'],how='left')\n",
    "supplier_product_mapping=pd.merge(supplier_location,BOM,on=['component_id'],how='left')\n",
    "supplier_product_portrisk=pd.merge(supplier_product_mapping,port_delay_df,on=['supplier_name','address'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplier_product_portrisk['lead_days'] = supplier_product_portrisk['creation_date'] - supplier_product_portrisk['needed_date']\n",
    "\n",
    "supplier_product_portrisk['days_difference'] = supplier_product_portrisk['promise_day_port_delay'] - supplier_product_portrisk['predicted/actual_days(back-up)']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in c:\\users\\tanish_sakate\\appdata\\roaming\\python\\python311\\site-packages (3.3.1)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in c:\\users\\tanish_sakate\\appdata\\roaming\\python\\python311\\site-packages (from pyspark) (0.10.9.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"OTR\").config(\"spark.sql.caseSensitive\", \"True\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ATISL225:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>OTR</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x213967729d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.json('csvjson (1).json')\n",
    "# Validating Type of Output\n",
    "# type(df)\n",
    "# spark = SparkSession.builder().getOrCreate()\n",
    "# df = spark.read.json(spark.sparkContext.wholeTextFiles('csvjson (1).json').values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "| tbl_port_congestion|\n",
      "+--------------------+\n",
      "|[{0.89, 0.54, 2.3...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# note : json format from database doesnt work\n",
    "multiline_df = spark.read.option(\"multiline\",\"true\") \\\n",
    "      .json(\"port_congestion.json\")\n",
    "multiline_df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.option(\"header\",True) \\\n",
    "     .csv(\"port_congestion.csv\")\n",
    "\n",
    "pandas_df = df2.toPandas()\n",
    "pandas_df=pandas_df[['port_id','elastic_loc_id','port_type','congestion_data_weekly_median_delay']]\n",
    "pandas_df.head(2)\n",
    "# elastic_loc_id,elastic_loc_id,port_type,congestion_data_weekly_median_delay,congestion_data_weekly_max_delay,congestion_data_fortnightly_median_delay\n",
    "# congestion_data_fortnightly_max_delay,congestion_data_monthly_median_delay,congestion_data_monthly_median_delay\n",
    "# df2 = spark.createDataFrame(pandas_df)\n",
    "# df2.show(1)\n",
    "schema = StructType([StructField(\"port_id\", StringType(), True), StructField(\"elastic_loc_id\", IntegerType(), True)\n",
    ", StructField(\"port_type\", StringType(), True),StructField(\"congestion_data_weekly_median_delay\", StringType(), True)])\n",
    "df = spark.createDataFrame(pandas_df, schema=schema)\n",
    "# df.show()\n",
    "# sparkDF=spark.createDataFrame(pandas_df) \n",
    "# sparkDF.printSchema()\n",
    "# sparkDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, port_id: int, port_display_name: string, elastic_loc_id: string, port_type: string, congestion_data_weekly_median_delay: double, congestion_data_weekly_max_delay: double, congestion_data_fortnightly_median_delay: double, congestion_data_fortnightly_max_delay: double, congestion_data_monthly_median_delay: double, congestion_data_monthly_max_delay: double, congestion_date: timestamp, created_at: timestamp, last_modified: timestamp]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
    " .csv(\"port_congestion.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05-01-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dateutil import parser\n",
    "import datetime\n",
    "import pandas as pd\n",
    "cal=parser.parse(\"20221211\")\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating date and month using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def create_date(x):\n",
    "   return parser.parse(x)\n",
    "def month_from_date(x):\n",
    "    return x.month\n",
    "def create_quarter(x):\n",
    "    return pd.to_datetime(x.month).quarter\n",
    "\n",
    "# val1=create_date('20221211')\n",
    "# val2=month_from_date(val1)\n",
    "# print(val2)\n",
    "# print(create_quarter(val1))\n",
    "\n",
    "df['date'] = df['Date'].astype(str)\n",
    "df[\"date\"] = df[\"date\"].apply(create_date)\n",
    "df[\"month\"] = df[\"date\"].apply(month_from_date)\n",
    "df[\"quarter\"] = df[\"date\"].apply(create_quarter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(val.month).quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adidas=pd.read_csv(r'D:\\AlixPartners\\adidas demo\\raw data\\adidas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Consignee City', 'Metric Tons', 'Consignee County',\n",
       "       'Final Destination', 'HS Description', 'Quantity', 'Quantity Unit',\n",
       "       'Country by Port of Departure', 'Weight', 'Port of Arrival', 'Date',\n",
       "       'Consignee State', 'Bill of Lading Nbr.', 'State of Port of Arrival',\n",
       "       'Port of Departure', 'Consignee Declared', 'Shipper Declared', 'row_id',\n",
       "       'Weight unit', 'Consignee Zip Code', 'Country of Origin',\n",
       "       'Shipper Declared Address'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adidas.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LOS ANGELES,CA                              29669\n",
       "CHARLESTON,SC                               24956\n",
       "NEW YORK/NEWARK AREA, NEWARK, NEW JERSEY    12314\n",
       "LONG BEACH,CA                               11953\n",
       "SAVANNAH,GA                                  5592\n",
       "HOUSTON,TX                                   2030\n",
       "PORT EVERGLADES,FL                            765\n",
       "BALTIMORE,MD                                  244\n",
       "SEATTLE,WA                                    221\n",
       "PHILADELPHIA,PA                               164\n",
       "Name: Port of Arrival, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adidas['Port of Arrival'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-25 00:00:00\n",
      "5\n",
      "5\n",
      "2022Q2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def create_date(x):\n",
    "   return parser.parse(x)\n",
    "def month_from_date(x):\n",
    "    # print(x.month)\n",
    "    return x.month\n",
    "def create_quarter(x):\n",
    "    print(pd.Period(x,'Q'))\n",
    "    return pd.to_datetime(x.month).quarter\n",
    "\n",
    "# val1=create_date('20220525')\n",
    "# print(val1)\n",
    "# val2=month_from_date(val1)\n",
    "# print(val2)\n",
    "# # hp2['Qtr'] = hp2['Mth'].map(lambda x: pd.Period(x,'Q'))\n",
    "# print(create_quarter(val1))\n",
    "df=adidas.copy()\n",
    "df['date'] = df['Date'].astype(str)\n",
    "df[\"date\"] = df[\"date\"].apply(create_date)\n",
    "df[\"month\"] = df[\"date\"].apply(month_from_date)\n",
    "df['quarter'] = [pd.Period(df.month[i], freq='M').quarter for i in range(len(df))]\n",
    "# df[\"quarter\"] = df[\"date\"].apply(create_quarter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20220404    511\n",
       "20220422    443\n",
       "20220525    428\n",
       "20220520    384\n",
       "20220508    366\n",
       "           ... \n",
       "20190903      1\n",
       "20190925      1\n",
       "20190923      1\n",
       "20190922      1\n",
       "20200528      1\n",
       "Name: Date, Length: 1786, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
